{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pNJD1mRv2iVc",
    "outputId": "ae2f25b9-acd1-4406-c36e-b9b06e0ed228"
   },
   "outputs": [],
   "source": [
    "pip install spacy vaderSentiment rake-nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "2lysGTeYcA9D"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import requests\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from rake_nltk import Rake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vsc00HcWcEJw",
    "outputId": "01420e5b-26bf-4b2c-e364-335eb8ede93b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\4hara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\4hara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\4hara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\4hara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\4hara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\4hara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "tF8SbF4jcHql"
   },
   "outputs": [],
   "source": [
    "def summarize_text(text):\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Calculate frequency of each word\n",
    "    word_frequencies = {}\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence.lower())\n",
    "        for word in words:\n",
    "            if word not in stopwords.words('english'):\n",
    "                if word not in word_frequencies:\n",
    "                    word_frequencies[word] = 1\n",
    "                else:\n",
    "                    word_frequencies[word] += 1\n",
    "\n",
    "    # Find weighted frequency of each word\n",
    "    max_frequency = max(word_frequencies.values())\n",
    "    for word in word_frequencies.keys():\n",
    "        word_frequencies[word] = (word_frequencies[word] / max_frequency)\n",
    "\n",
    "    # Calculate sentence scores based on word frequencies\n",
    "    sentence_scores = {}\n",
    "    for sentence in sentences:\n",
    "        for word in word_tokenize(sentence.lower()):\n",
    "            if word in word_frequencies.keys():\n",
    "                if len(sentence.split(' ')) < 30:  # Limiting sentence length\n",
    "                    if sentence not in sentence_scores.keys():\n",
    "                        sentence_scores[sentence] = word_frequencies[word]\n",
    "                    else:\n",
    "                        sentence_scores[sentence] += word_frequencies[word]\n",
    "\n",
    "    # Get the top 3 sentences as the summary\n",
    "    summary_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)[:3]\n",
    "    summary = ' '.join(summary_sentences)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "3Znp-JD-cMDX"
   },
   "outputs": [],
   "source": [
    "def analyze_bias_and_sentiment(summary):\n",
    "    # Sentiment Analysis\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    sentiment_score = sid.polarity_scores(summary)\n",
    "    sentiment = \"Positive\" if sentiment_score['compound'] >= 0 else \"Negative\"\n",
    "\n",
    "    return sentiment, sentiment_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EZaOo-Dz0FrT",
    "outputId": "e27a5246-119b-43cc-98d7-cefc9ea79208"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Paste your URL here. https://uk.news.yahoo.com/justice-dept-makes-arrests-north-201310276.html\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WASHINGTON (AP) — The Justice Department announced Thursday multiple arrests in a series of complex stolen identity theft cases that officials say are part of a wide-ranging scheme that generates enormous proceeds for the North Korean government, including for its weapons program.\n",
      "The conspiracy involves thousands of North Korean information technology workers who prosecutors say are dispatched by the government to live abroad and who rely on the stolen identities of Americas to obtain remote employment at U.S.-based Fortune 500 companies, jobs that give them access to sensitive corporate data and lucrative paychecks. The companies did not realize the workers were overseas.\n",
      "The fraud scheme is a way for heavily sanctioned North Korea, which is cut off from the U.S. financial system, to take advantage of a “toxic brew” of converging factors, including a high-tech labor shortage in the U.S. and the proliferation of remote telework, Marshall Miller, the Justice Department's principal associate deputy attorney general, said in an interview.\n",
      "The Justice Department says the cases are part of a broader strategy to not only prosecute individuals who enable the fraud but also to build partnerships with other countries and to warn private-sector companies of the need to be vigilant — and not duped — about the actual identities of the people they're hiring.\n",
      "FBI and Justice Department officials launched an initiative in March centered on the fraud scheme and last year announced the seizure of more than a dozen website domains used by North Korean IT workers.\n",
      "“More and more often, compliance programs at American companies and organizations are on the front lines of protecting our national security,” Miller said. \"Corporate compliance and national security are now intertwined like never before.”\n",
      "The Justice Department said in court documents in one case that more than 300 companies — including a high-end retail chain and a “premier Silicon Valley technology company” — have been affected and that more than $6.8 million in revenue has been generated for the workers, who are based outside of the U.S., including in China and Russia.\n",
      "Those arrested include an Arizona woman, Christina Marie Chapman, who prosecutors say facilitated the scheme by helping the workers obtain and validate stolen identities, receiving and hosting laptops from U.S. companies who thought they were sending the devices to legitimate employees and helping the workers connect remotely to companies.\n",
      "According to the indictment, Chapman ran more than one “laptop farm” where U.S. companies sent computers and paychecks to IT workers they did not realize were overseas.\n",
      "At Chapman’s laptop farms, she allegedly connected overseas IT workers who logged in remotely to company networks so it appeared the logins were coming from the United States. She also is alleged to have received paychecks for the overseas IT workers at her home, forging the beneficiaries’ signatures for transfer abroad and enriching herself by charging monthly fees.\n",
      "Other defendants include a Ukrainian man, Oleksandr Didenko, who prosecutors say created fake accounts at job search platforms that he then sold to overseas workers who went on to apply for jobs at U.S. companies. He was was arrested in Poland last week, and the Justice Department said it had seized his company's online domain.\n",
      "A Vietnamese national, Minh Phuong Vong, was arrested in Maryland on charges of fraudulently obtaining a job at a U.S. company that was actually performed by remote workers who posed as him and were based overseas.\n",
      "It was not immediately clear if any of the three had lawyers.\n",
      "Separately, the State Department said it was offering a reward for information about certain North Korean IT workers who officials say were assisted by Chapman.\n",
      "And the FBI, which conducted the investigations, issued a public service announcement that warned companies about the scheme, encouraging them to implement identity verification standards through the hiring process and to educate human resources staff and hiring managers about the threat.\n",
      "____\n",
      "Associated Press writer Frank Bajak in Boston contributed to the report.\n"
     ]
    }
   ],
   "source": [
    "def extract_article_content(url, token):\n",
    "    # Construct the API request URL\n",
    "    api_url = f\"https://api.diffbot.com/v3/article\"\n",
    "    params = {\n",
    "        'token': token,\n",
    "        'url': url,\n",
    "    }\n",
    "\n",
    "    # Send the API request\n",
    "    response = requests.get(api_url, params=params)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the JSON response\n",
    "        data = response.json()\n",
    "\n",
    "        # Check if article extraction was successful\n",
    "        if data.get('objects'):\n",
    "            article_content = data['objects'][0]['text']\n",
    "            return article_content\n",
    "        else:\n",
    "            return \"Failed to extract article content.\"\n",
    "    else:\n",
    "        return \"Failed to retrieve data from Diffbot.\"\n",
    "\n",
    "# Example URL and API token\n",
    "url = input(\"Paste your URL here.\")\n",
    "token = \"72396479255cf4be659af8c87bd3d3f0\"  # Replace this with your Diffbot API token\n",
    "\n",
    "# Extract article contenthttps://housker.github.io/confidentscientist/es/posts/24-05-15/\n",
    "article_content = extract_article_content(url, token)\n",
    "print(article_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article Title: Justice Dept. makes arrests in North Korean identity theft scheme involving thousands of IT workers\n"
     ]
    }
   ],
   "source": [
    "def get_article_title(url, token):\n",
    "# Construct the API request URL\n",
    "    api_url = f\"https://api.diffbot.com/v3/article\"\n",
    "    params = {\n",
    "        'token': token,\n",
    "        'url': url,\n",
    "    }\n",
    "\n",
    "    # Send the API request\n",
    "    response = requests.get(api_url, params=params)\n",
    "\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        # Extract the title from the response\n",
    "        if 'objects' in data and len(data['objects']) > 0:\n",
    "            title = data['objects'][0].get('title', 'No title found')\n",
    "            return title\n",
    "        else:\n",
    "            return 'No article objects found in the response'\n",
    "    else:\n",
    "        return f'Error: {response.status_code}'\n",
    "\n",
    "\n",
    "title = get_article_title(url, token)\n",
    "print(\"Article Title:\", title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4aBd-YspcQS_",
    "outputId": "d313e901-0e26-44fa-86ee-9f0f9413b44a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "“More and more often, compliance programs at American companies and organizations are on the front lines of protecting our national security,” Miller said. According to the indictment, Chapman ran more than one “laptop farm” where U.S. companies sent computers and paychecks to IT workers they did not realize were overseas. Separately, the State Department said it was offering a reward for information about certain North Korean IT workers who officials say were assisted by Chapman.\n",
      "\n",
      "Sentiment: Positive\n",
      "Sentiment Score: {'neg': 0.0, 'neu': 0.926, 'pos': 0.074, 'compound': 0.7003}\n"
     ]
    }
   ],
   "source": [
    "#Text Input\n",
    "input_text = article_content\n",
    "\n",
    "# Summarize the input text\n",
    "summary = summarize_text(input_text)\n",
    "\n",
    "# Analyze bias and sentiment of the summary\n",
    "sentiment, sentiment_score = analyze_bias_and_sentiment(summary)\n",
    "\n",
    "print(\"Summary:\")\n",
    "print(summary)\n",
    "print(\"\\nSentiment:\", sentiment)\n",
    "print(\"Sentiment Score:\", sentiment_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "itTtDjxxzLQ2",
    "outputId": "a521afe0-2eb9-4131-fb4c-6041f298b57b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic Keywords: ['300', '500', '____', 'access', 'according']\n",
      "\n",
      "Summary:\n",
      "the conspiracy involves thousands of north korean information technology workers who prosecutors say are dispatched by the government to live abroad and who rely on the stolen identities of americas to obtain remote employment at u.s.-based fortune 500 companies, jobs that give them access to sensitive corporate data and lucrative paychecks. \"corporate compliance and national security are now intertwined like never before.”\n",
      "the justice department said in court documents in one case that more than 300 companies — including a high-end retail chain and a “premier silicon valley technology company” — have been affected and that more than $6.8 million in revenue has been generated for the workers, who are based outside of the u.s., including in china and russia. according to the indictment, chapman ran more than one “laptop farm” where u.s. companies sent computers and paychecks to it workers they did not realize were overseas. ____\n",
      "associated press writer frank bajak in boston contributed to the report.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from heapq import nlargest\n",
    "\n",
    "def analyze_topic_and_summarize(text):\n",
    "    # Tokenize the text into sentences and words\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Tokenize words and filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = [word.lower() for word in word_tokenize(text) if word.isalnum() and word.lower() not in stop_words]\n",
    "\n",
    "    # Convert sentences to lower case\n",
    "    sentences = [sentence.lower() for sentence in sentences]\n",
    "\n",
    "    # Calculate TF-IDF scores for words\n",
    "    tfidf = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf.fit_transform(sentences)\n",
    "    word_scores = dict(zip(tfidf.get_feature_names_out(), tfidf.idf_))\n",
    "\n",
    "    # Get the top keywords based on TF-IDF scores\n",
    "    top_keywords = nlargest(5, word_scores, key=word_scores.get)\n",
    "\n",
    "    # Generate summary sentences containing the top keywords\n",
    "    summary_sentences = []\n",
    "    for sentence in sentences:\n",
    "        if any(keyword in sentence for keyword in top_keywords):\n",
    "            summary_sentences.append(sentence)\n",
    "\n",
    "    # Combine summary sentences into a single summary\n",
    "    summary = ' '.join(summary_sentences)\n",
    "\n",
    "    return top_keywords, summary\n",
    "\n",
    "# Example input text\n",
    "input_text = article_content\n",
    "\n",
    "# Analyze the topic and summarize the input text\n",
    "top_keywords, summary = analyze_topic_and_summarize(input_text)\n",
    "print(\"Topic Keywords:\", top_keywords)\n",
    "print(\"\\nSummary:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "70Xd1O3Wp6KA",
    "outputId": "5bc6f2e7-5826-4847-fe16-8cf71da9e5d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary:\n",
      "({'AP', 'WASHINGTON', 'Department', 'Korean', 'Justice'}, 'WASHINGTON (AP) — The Justice Department announced Thursday multiple arrests in a series of complex stolen identity theft cases that officials say are part of a wide-ranging scheme that generates enormous proceeds for the North Korean government, including for its weapons program. The conspiracy involves thousands of North Korean information technology workers who prosecutors say are dispatched by the government to live abroad and who rely on the stolen identities of Americas to obtain remote employment at U.S.-based Fortune 500 companies, jobs that give them access to sensitive corporate data and lucrative paychecks. The fraud scheme is a way for heavily sanctioned North Korea, which is cut off from the U.S. financial system, to take advantage of a “toxic brew” of converging factors, including a high-tech labor shortage in the U.S. and the proliferation of remote telework, Marshall Miller, the Justice Department\\'s principal associate deputy attorney general, said in an interview. The Justice Department says the cases are part of a broader strategy to not only prosecute individuals who enable the fraud but also to build partnerships with other countries and to warn private-sector companies of the need to be vigilant — and not duped — about the actual identities of the people they\\'re hiring. FBI and Justice Department officials launched an initiative in March centered on the fraud scheme and last year announced the seizure of more than a dozen website domains used by North Korean IT workers. \"Corporate compliance and national security are now intertwined like never before.”\\nThe Justice Department said in court documents in one case that more than 300 companies — including a high-end retail chain and a “premier Silicon Valley technology company” — have been affected and that more than $6.8 million in revenue has been generated for the workers, who are based outside of the U.S., including in China and Russia. Those arrested include an Arizona woman, Christina Marie Chapman, who prosecutors say facilitated the scheme by helping the workers obtain and validate stolen identities, receiving and hosting laptops from U.S. companies who thought they were sending the devices to legitimate employees and helping the workers connect remotely to companies. According to the indictment, Chapman ran more than one “laptop farm” where U.S. companies sent computers and paychecks to IT workers they did not realize were overseas. At Chapman’s laptop farms, she allegedly connected overseas IT workers who logged in remotely to company networks so it appeared the logins were coming from the United States. Other defendants include a Ukrainian man, Oleksandr Didenko, who prosecutors say created fake accounts at job search platforms that he then sold to overseas workers who went on to apply for jobs at U.S. companies. He was was arrested in Poland last week, and the Justice Department said it had seized his company\\'s online domain. Separately, the State Department said it was offering a reward for information about certain North Korean IT workers who officials say were assisted by Chapman.')\n",
      "\n",
      "Sentiment: Positive\n",
      "Sentiment Score: {'neg': 0.0, 'neu': 0.926, 'pos': 0.074, 'compound': 0.7003}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "\n",
    "def analyze_topic_and_summarize(text):\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Initialize an empty list to store relevant entities\n",
    "    relevant_entities = []\n",
    "\n",
    "    # Perform part-of-speech tagging and named entity recognition for each sentence\n",
    "    for sentence in sentences:\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        tagged_words = nltk.pos_tag(words)\n",
    "        named_entities = nltk.ne_chunk(tagged_words)\n",
    "\n",
    "        # Extract person entities and other relevant entities (organizations, geopolitical entities)\n",
    "        for entity in named_entities:\n",
    "            if isinstance(entity, nltk.tree.Tree):\n",
    "                if entity.label() in [\"PERSON\", \"ORGANIZATION\", \"GPE\"]:\n",
    "                    relevant_entities.extend([word for word, _ in entity.leaves()])\n",
    "\n",
    "    # Get the top keywords\n",
    "    top_keywords = set(relevant_entities[:5])  # Adjust as needed\n",
    "\n",
    "    # Generate summary sentences containing the top keywords\n",
    "    summary_sentences = []\n",
    "    for sentence in sentences:\n",
    "        if any(keyword.lower() in sentence.lower() for keyword in top_keywords):\n",
    "            summary_sentences.append(sentence)\n",
    "\n",
    "    # Combine summary sentences into a single summary\n",
    "    summary = ' '.join(summary_sentences)\n",
    "\n",
    "    return top_keywords, summary\n",
    "\n",
    "\n",
    "# Analyze the topic and summarize the input text\n",
    "summary = analyze_topic_and_summarize(article_content)\n",
    "# print(\"Topic Keywords:\", top_keywords)\n",
    "print(\"\\nSummary:\")\n",
    "print(summary)\n",
    "print(\"\\nSentiment:\", sentiment)\n",
    "print(\"Sentiment Score:\", sentiment_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "GvE3GFrfvWfK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Sentiment Label towards the Idea: Supportive\n",
      "Sentiment towards Entities:\n",
      "\n",
      "Summary:\n",
      "the conspiracy involves thousands of north korean information technology workers who prosecutors say are dispatched by the government to live abroad and who rely on the stolen identities of americas to obtain remote employment at u.s.-based fortune 500 companies, jobs that give them access to sensitive corporate data and lucrative paychecks. \"corporate compliance and national security are now intertwined like never before.”\n",
      "the justice department said in court documents in one case that more than 300 companies — including a high-end retail chain and a “premier silicon valley technology company” — have been affected and that more than $6.8 million in revenue has been generated for the workers, who are based outside of the u.s., including in china and russia. according to the indictment, chapman ran more than one “laptop farm” where u.s. companies sent computers and paychecks to it workers they did not realize were overseas. ____\n",
      "associated press writer frank bajak in boston contributed to the report.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Load the English language model for SpaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize the VADER sentiment analyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    # Perform sentiment analysis using VADER\n",
    "    sentiment_scores = sid.polarity_scores(text)\n",
    "    compound_score = sentiment_scores['compound']\n",
    "\n",
    "    # Determine sentiment label based on compound score\n",
    "    sentiment = \"Positive\" if compound_score >= 0.05 else \"Negative\" if compound_score <= -0.05 else \"Neutral\"\n",
    "    return sentiment\n",
    "\n",
    "def analyze_for_or_against_idea(text, idea):\n",
    "    # Process the text using SpaCy\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Extract entities and analyze sentiment towards them\n",
    "    entities_sentiment = {}\n",
    "    for ent in doc.ents:\n",
    "        if ent.text.lower() == idea.lower():\n",
    "            continue  # Skip the idea itself\n",
    "        entity_sentiment = analyze_sentiment(ent.text)\n",
    "        entities_sentiment[ent.text] = entity_sentiment\n",
    "\n",
    "    # Determine overall sentiment towards the idea\n",
    "    idea_sentiment = analyze_sentiment(idea)\n",
    "\n",
    "    # Determine if the overall sentiment is for or against the idea\n",
    "    if idea_sentiment == \"Positive\":\n",
    "        sentiment_label = \"Supportive\"\n",
    "    elif idea_sentiment == \"Negative\":\n",
    "        sentiment_label = \"Critical\"\n",
    "    else:\n",
    "        sentiment_label = \"Neutral\"\n",
    "\n",
    "    return sentiment_label, entities_sentiment\n",
    "\n",
    "# Example idea\n",
    "idea = title\n",
    "\n",
    "# Analyze sentiment towards the idea and entities mentioned in the text\n",
    "sentiment_label, entities_sentiment = analyze_for_or_against_idea(article_content, idea)\n",
    "\n",
    "# Print results\n",
    "print(\"Overall Sentiment Label towards the Idea:\", sentiment_label)\n",
    "print(\"Sentiment towards Entities:\")\n",
    "print(\"\\nSummary:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
