+++
title = 'Babel'
date = 2024-05-14T11:12:37-10:00
featured_image = 'images/24-05-14.webp'
+++

The classmates I'm doing my final project with want to quit. We're trying to [train](https://kaitchup.substack.com/p/datasets-to-train-validate-and-evaluate-machine-translation-d61905d126aa) an LLM to translate text, and the [dataset](https://www.kaggle.com/datasets/ramakrishnan1984/785-million-language-translation-database-ai-ml) it too large for our CPU and RAM to handle. I don't want to quit. Knowing what to do with large data sets is enormously important. I'll give feedback that they ought to include that in the curriculum. So far neither the teacher nor the TAs know what to do, but I'm sure we can figure it out.

Once we train our own model, I'll replace [Google's API](https://cloud.google.com/translate?hl=en), which I'm using currently to [generate](https://github.com/housker/blog/blob/main/translate/translate.go) Spanish, French, and Norwegian content from my English posts.

There is an [advanced](https://cloud.google.com/translate/docs/intro-to-v3) version of the API I am currently using that does permit custom models. It is likely integrated with Google [Cloud Storage](https://cloud.google.com/storage/docs/samples/storage-transfer-manager-download-chunks-concurrently). That would be a good idea for production if budget permits, but for the purposes of education, I'm inclined to build one from scratch for practice, though the accuracy would likely be poorer and the infrastructure less polished. [Pandas](https://pandas.pydata.org/docs/user_guide/scale.html) and [Scikit](https://scikit-learn.org/stable/computing/scaling_strategies.html) provide guides for dealing with large datasets. Perplexity recommends using [Dataflow](https://cloud.google.com/dataflow?hl=en) and Apache Beam, but if you follow the [link](https://huggingface.co/docs/datasets/en/filesystems), you'll see it's deprecated in Hugging Face. The website does list [alternatives](https://huggingface.co/docs/datasets/en/filesystems). Polars is mentioned in [Reddit](https://www.reddit.com/r/dataengineering/comments/13n2f63/how_to_process_data_larger_than_memory_using_a/). Interestingly, that is not mentioned as a recommended [library](https://pandas.pydata.org/community/ecosystem.html#out-of-core) by Pandas, though Dask is, and is mentioned in this [StackOverflow](https://stackoverflow.com/questions/56823593/how-to-read-chunks-of-multiple-large-csv-files-from-google-cloud-storage-using-d) post.

I can't figure out why they were ever punished for building that tower.